{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a0ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 18:21:52.423733: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-12 18:21:52.627863: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-12 18:21:52.629271: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-12 18:21:58.642584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "# Used in FL\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import collections\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10351f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 18:22:15.627943: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-04-12 18:22:15.629078: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "E0412 18:22:15.880977900     207 socket_utils_common_posix.cc:221] check for SO_REUSEPORT: {\"created\":\"@1681338135.880942100\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":199,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "E0412 18:22:15.881804100     207 socket_utils_common_posix.cc:327] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, World!'\n"
     ]
    }
   ],
   "source": [
    "# Testing TFF\n",
    "# tff.backends.native.set_local_python_execution_context()\n",
    "print(tff.federated_computation(lambda: 'Hello, World!')())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e4ab1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files into separate dataframes\n",
    "df_ey = pd.read_csv('/home/draxtik20/Federated_Learning_CDI/Datasets/EIRLI_younger.csv')\n",
    "df_eo = pd.read_csv('/home/draxtik20/Federated_Learning_CDI/Datasets/EIRLI_older.csv')\n",
    "df_ly = pd.read_csv('/home/draxtik20/Federated_Learning_CDI/Datasets/LASER_younger.csv')\n",
    "df_lo = pd.read_csv('/home/draxtik20/Federated_Learning_CDI/Datasets/LASER_older.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a1cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes into one dataframe with matching headers\n",
    "df = pd.concat([df_ey, df_eo, df_ly, df_lo], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d782074c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level.gender</th>\n",
       "      <th>Level.ethnic</th>\n",
       "      <th>Level.med</th>\n",
       "      <th>Level.fed</th>\n",
       "      <th>Level.income</th>\n",
       "      <th>FamHist</th>\n",
       "      <th>Percentile</th>\n",
       "      <th>GCC</th>\n",
       "      <th>Degree</th>\n",
       "      <th>HC</th>\n",
       "      <th>Between</th>\n",
       "      <th>PathLength</th>\n",
       "      <th>CombWords</th>\n",
       "      <th>AnyLangorReadDxOnly</th>\n",
       "      <th>GramComplex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>3.632653</td>\n",
       "      <td>0.258744</td>\n",
       "      <td>27.061224</td>\n",
       "      <td>2.870240</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.597950</td>\n",
       "      <td>12.480000</td>\n",
       "      <td>0.336051</td>\n",
       "      <td>144.480000</td>\n",
       "      <td>2.987580</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.212454</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.615385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.543568</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>0.293187</td>\n",
       "      <td>128.159722</td>\n",
       "      <td>3.270546</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.691011</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.140688</td>\n",
       "      <td>5.416667</td>\n",
       "      <td>2.266234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.516904</td>\n",
       "      <td>17.176471</td>\n",
       "      <td>0.319899</td>\n",
       "      <td>300.974790</td>\n",
       "      <td>3.040681</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.933333</td>\n",
       "      <td>0.106475</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>2.823529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dx</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.565451</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.310525</td>\n",
       "      <td>191.393162</td>\n",
       "      <td>3.060358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.547224</td>\n",
       "      <td>12.713043</td>\n",
       "      <td>0.319673</td>\n",
       "      <td>182.330435</td>\n",
       "      <td>2.966610</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>874 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Level.gender  Level.ethnic  Level.med  Level.fed  Level.income  FamHist  \\\n",
       "0               0             4          4          4             3        0   \n",
       "1               1             4          6          6             3        0   \n",
       "2               1             4          5          4             2        1   \n",
       "3               0             4          3          3             2        0   \n",
       "4               1             4          6          3             3        0   \n",
       "..            ...           ...        ...        ...           ...      ...   \n",
       "869             0             4          6          4             3        1   \n",
       "870             0             4          4          6             3        0   \n",
       "871             1             4          5          3             2        0   \n",
       "872             1             1          3          2             1        1   \n",
       "873             0             4          3          4             2        0   \n",
       "\n",
       "     Percentile       GCC     Degree        HC     Between  PathLength  \\\n",
       "0          67.0  0.431818   3.632653  0.258744   27.061224    2.870240   \n",
       "1          95.0  0.597950  12.480000  0.336051  144.480000    2.987580   \n",
       "2          10.0  0.000000   1.000000  0.416667    0.250000    1.333333   \n",
       "3          37.0  0.473684   2.000000  0.212454    1.142857    1.615385   \n",
       "4          92.0  0.543568   8.125000  0.293187  128.159722    3.270546   \n",
       "..          ...       ...        ...       ...         ...         ...   \n",
       "869         1.0  0.691011   2.666667  0.140688    5.416667    2.266234   \n",
       "870        95.0  0.516904  17.176471  0.319899  300.974790    3.040681   \n",
       "871         1.0  0.818182   1.933333  0.106475    5.166667    2.823529   \n",
       "872        48.0  0.565451  13.000000  0.310525  191.393162    3.060358   \n",
       "873        41.0  0.547224  12.713043  0.319673  182.330435    2.966610   \n",
       "\n",
       "     CombWords AnyLangorReadDxOnly  GramComplex  \n",
       "0          2.0                NoDx          NaN  \n",
       "1          1.0                NoDx          NaN  \n",
       "2          1.0                NoDx          NaN  \n",
       "3          1.0                NoDx          NaN  \n",
       "4          2.0                NoDx          NaN  \n",
       "..         ...                 ...          ...  \n",
       "869        1.0                NoDx         15.0  \n",
       "870        1.0                NoDx         55.0  \n",
       "871        1.0                  Dx         39.0  \n",
       "872        1.0                NoDx         55.0  \n",
       "873        1.0                NoDx         45.0  \n",
       "\n",
       "[874 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4dc6176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique client ID for each row\n",
    "df['client_id'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "\n",
    "# Set the index of the dataframe to the client_id column\n",
    "df.set_index('client_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3046545f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level.gender</th>\n",
       "      <th>Level.ethnic</th>\n",
       "      <th>Level.med</th>\n",
       "      <th>Level.fed</th>\n",
       "      <th>Level.income</th>\n",
       "      <th>FamHist</th>\n",
       "      <th>Percentile</th>\n",
       "      <th>GCC</th>\n",
       "      <th>Degree</th>\n",
       "      <th>HC</th>\n",
       "      <th>Between</th>\n",
       "      <th>PathLength</th>\n",
       "      <th>CombWords</th>\n",
       "      <th>AnyLangorReadDxOnly</th>\n",
       "      <th>GramComplex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23efe735-c134-4ffd-841b-e67e3f618d28</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>3.632653</td>\n",
       "      <td>0.258744</td>\n",
       "      <td>27.061224</td>\n",
       "      <td>2.870240</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405d15dc-f781-4aed-abcb-288b2674dc38</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.597950</td>\n",
       "      <td>12.480000</td>\n",
       "      <td>0.336051</td>\n",
       "      <td>144.480000</td>\n",
       "      <td>2.987580</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2fd0705a-a8fe-49db-9426-e6fc7e9378f6</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a46ea600-2cda-428e-aa8c-8f35ca84abb0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.212454</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.615385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697524fa-f10a-42ed-9ed3-52eaee0f1b32</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.543568</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>0.293187</td>\n",
       "      <td>128.159722</td>\n",
       "      <td>3.270546</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8fb6410d-989d-4321-8cb3-d952001a2216</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.691011</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.140688</td>\n",
       "      <td>5.416667</td>\n",
       "      <td>2.266234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1c9a021-7770-4834-b7ce-49da4a0151d7</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.516904</td>\n",
       "      <td>17.176471</td>\n",
       "      <td>0.319899</td>\n",
       "      <td>300.974790</td>\n",
       "      <td>3.040681</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542bc78d-d5a2-44b2-ad27-44b879f852b1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.933333</td>\n",
       "      <td>0.106475</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>2.823529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dx</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d8667941-4048-49d8-a1c7-5a7c138cba6c</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.565451</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.310525</td>\n",
       "      <td>191.393162</td>\n",
       "      <td>3.060358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d53d2f9b-0577-48ff-a5cf-9020950e52f7</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.547224</td>\n",
       "      <td>12.713043</td>\n",
       "      <td>0.319673</td>\n",
       "      <td>182.330435</td>\n",
       "      <td>2.966610</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NoDx</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>874 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Level.gender  Level.ethnic  Level.med  \\\n",
       "client_id                                                                     \n",
       "23efe735-c134-4ffd-841b-e67e3f618d28             0             4          4   \n",
       "405d15dc-f781-4aed-abcb-288b2674dc38             1             4          6   \n",
       "2fd0705a-a8fe-49db-9426-e6fc7e9378f6             1             4          5   \n",
       "a46ea600-2cda-428e-aa8c-8f35ca84abb0             0             4          3   \n",
       "697524fa-f10a-42ed-9ed3-52eaee0f1b32             1             4          6   \n",
       "...                                            ...           ...        ...   \n",
       "8fb6410d-989d-4321-8cb3-d952001a2216             0             4          6   \n",
       "f1c9a021-7770-4834-b7ce-49da4a0151d7             0             4          4   \n",
       "542bc78d-d5a2-44b2-ad27-44b879f852b1             1             4          5   \n",
       "d8667941-4048-49d8-a1c7-5a7c138cba6c             1             1          3   \n",
       "d53d2f9b-0577-48ff-a5cf-9020950e52f7             0             4          3   \n",
       "\n",
       "                                      Level.fed  Level.income  FamHist  \\\n",
       "client_id                                                                \n",
       "23efe735-c134-4ffd-841b-e67e3f618d28          4             3        0   \n",
       "405d15dc-f781-4aed-abcb-288b2674dc38          6             3        0   \n",
       "2fd0705a-a8fe-49db-9426-e6fc7e9378f6          4             2        1   \n",
       "a46ea600-2cda-428e-aa8c-8f35ca84abb0          3             2        0   \n",
       "697524fa-f10a-42ed-9ed3-52eaee0f1b32          3             3        0   \n",
       "...                                         ...           ...      ...   \n",
       "8fb6410d-989d-4321-8cb3-d952001a2216          4             3        1   \n",
       "f1c9a021-7770-4834-b7ce-49da4a0151d7          6             3        0   \n",
       "542bc78d-d5a2-44b2-ad27-44b879f852b1          3             2        0   \n",
       "d8667941-4048-49d8-a1c7-5a7c138cba6c          2             1        1   \n",
       "d53d2f9b-0577-48ff-a5cf-9020950e52f7          4             2        0   \n",
       "\n",
       "                                      Percentile       GCC     Degree  \\\n",
       "client_id                                                               \n",
       "23efe735-c134-4ffd-841b-e67e3f618d28        67.0  0.431818   3.632653   \n",
       "405d15dc-f781-4aed-abcb-288b2674dc38        95.0  0.597950  12.480000   \n",
       "2fd0705a-a8fe-49db-9426-e6fc7e9378f6        10.0  0.000000   1.000000   \n",
       "a46ea600-2cda-428e-aa8c-8f35ca84abb0        37.0  0.473684   2.000000   \n",
       "697524fa-f10a-42ed-9ed3-52eaee0f1b32        92.0  0.543568   8.125000   \n",
       "...                                          ...       ...        ...   \n",
       "8fb6410d-989d-4321-8cb3-d952001a2216         1.0  0.691011   2.666667   \n",
       "f1c9a021-7770-4834-b7ce-49da4a0151d7        95.0  0.516904  17.176471   \n",
       "542bc78d-d5a2-44b2-ad27-44b879f852b1         1.0  0.818182   1.933333   \n",
       "d8667941-4048-49d8-a1c7-5a7c138cba6c        48.0  0.565451  13.000000   \n",
       "d53d2f9b-0577-48ff-a5cf-9020950e52f7        41.0  0.547224  12.713043   \n",
       "\n",
       "                                            HC     Between  PathLength  \\\n",
       "client_id                                                                \n",
       "23efe735-c134-4ffd-841b-e67e3f618d28  0.258744   27.061224    2.870240   \n",
       "405d15dc-f781-4aed-abcb-288b2674dc38  0.336051  144.480000    2.987580   \n",
       "2fd0705a-a8fe-49db-9426-e6fc7e9378f6  0.416667    0.250000    1.333333   \n",
       "a46ea600-2cda-428e-aa8c-8f35ca84abb0  0.212454    1.142857    1.615385   \n",
       "697524fa-f10a-42ed-9ed3-52eaee0f1b32  0.293187  128.159722    3.270546   \n",
       "...                                        ...         ...         ...   \n",
       "8fb6410d-989d-4321-8cb3-d952001a2216  0.140688    5.416667    2.266234   \n",
       "f1c9a021-7770-4834-b7ce-49da4a0151d7  0.319899  300.974790    3.040681   \n",
       "542bc78d-d5a2-44b2-ad27-44b879f852b1  0.106475    5.166667    2.823529   \n",
       "d8667941-4048-49d8-a1c7-5a7c138cba6c  0.310525  191.393162    3.060358   \n",
       "d53d2f9b-0577-48ff-a5cf-9020950e52f7  0.319673  182.330435    2.966610   \n",
       "\n",
       "                                      CombWords AnyLangorReadDxOnly  \\\n",
       "client_id                                                             \n",
       "23efe735-c134-4ffd-841b-e67e3f618d28        2.0                NoDx   \n",
       "405d15dc-f781-4aed-abcb-288b2674dc38        1.0                NoDx   \n",
       "2fd0705a-a8fe-49db-9426-e6fc7e9378f6        1.0                NoDx   \n",
       "a46ea600-2cda-428e-aa8c-8f35ca84abb0        1.0                NoDx   \n",
       "697524fa-f10a-42ed-9ed3-52eaee0f1b32        2.0                NoDx   \n",
       "...                                         ...                 ...   \n",
       "8fb6410d-989d-4321-8cb3-d952001a2216        1.0                NoDx   \n",
       "f1c9a021-7770-4834-b7ce-49da4a0151d7        1.0                NoDx   \n",
       "542bc78d-d5a2-44b2-ad27-44b879f852b1        1.0                  Dx   \n",
       "d8667941-4048-49d8-a1c7-5a7c138cba6c        1.0                NoDx   \n",
       "d53d2f9b-0577-48ff-a5cf-9020950e52f7        1.0                NoDx   \n",
       "\n",
       "                                      GramComplex  \n",
       "client_id                                          \n",
       "23efe735-c134-4ffd-841b-e67e3f618d28          NaN  \n",
       "405d15dc-f781-4aed-abcb-288b2674dc38          NaN  \n",
       "2fd0705a-a8fe-49db-9426-e6fc7e9378f6          NaN  \n",
       "a46ea600-2cda-428e-aa8c-8f35ca84abb0          NaN  \n",
       "697524fa-f10a-42ed-9ed3-52eaee0f1b32          NaN  \n",
       "...                                           ...  \n",
       "8fb6410d-989d-4321-8cb3-d952001a2216         15.0  \n",
       "f1c9a021-7770-4834-b7ce-49da4a0151d7         55.0  \n",
       "542bc78d-d5a2-44b2-ad27-44b879f852b1         39.0  \n",
       "d8667941-4048-49d8-a1c7-5a7c138cba6c         55.0  \n",
       "d53d2f9b-0577-48ff-a5cf-9020950e52f7         45.0  \n",
       "\n",
       "[874 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e80cb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GramComplex']\n"
     ]
    }
   ],
   "source": [
    "# Get the columns with NaN values\n",
    "nan_cols = df.columns[df.isnull().any()]\n",
    "\n",
    "# Print the columns with NaN values as a list\n",
    "print(nan_cols.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f133a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "# Fill missing values in the numeric columns with the mean value\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6517fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the 'NoDx' value with 0 and the 'Dx' value with 1\n",
    "df[\"AnyLangorReadDxOnly\"] = df[\"AnyLangorReadDxOnly\"].replace(\"NoDx\", 0).replace(\"Dx\", 1)\n",
    "df[\"AnyLangorReadDxOnly\"] = df[\"AnyLangorReadDxOnly\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "175e2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08eadc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699 175 <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data), type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44eff724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level.gender             int64\n",
      "Level.ethnic             int64\n",
      "Level.med                int64\n",
      "Level.fed                int64\n",
      "Level.income             int64\n",
      "FamHist                  int64\n",
      "Percentile             float64\n",
      "GCC                    float64\n",
      "Degree                 float64\n",
      "HC                     float64\n",
      "Between                float64\n",
      "PathLength             float64\n",
      "CombWords              float64\n",
      "AnyLangorReadDxOnly      int64\n",
      "GramComplex            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b558313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=({'Level.gender': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'Level.ethnic': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'Level.med': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'Level.fed': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'Level.income': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'FamHist': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'Percentile': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'GCC': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'Degree': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'HC': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'Between': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'PathLength': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'CombWords': TensorSpec(shape=(None,), dtype=tf.float64, name=None), 'GramComplex': TensorSpec(shape=(None,), dtype=tf.float64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Convert the training data to a TensorFlow Dataset\n",
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_data, label=\"AnyLangorReadDxOnly\")\n",
    "print(train_ds)#.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------Federated Learning Model------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d5bf8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 10\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 20\n",
    "SHUFFLE_BUFFER = 100\n",
    "PREFETCH_BUFFER = 10\n",
    "\n",
    "def preprocess(dataset):\n",
    "\n",
    "  def batch_format_fn(element):\n",
    "    \n",
    "    return collections.OrderedDict(\n",
    "        # Reshape pixels for this Digit from a 28x28 2D array, into a 1D array with 784 pixels\n",
    "        # and return the (Label, [Pixels]) as an `OrderedDict`.\n",
    "        x=tf.reshape(element['Level.gender'], [-1, 784]),\n",
    "        y=tf.reshape(element['label'], [-1, 1]))\n",
    "  # Return reformated, batched, shuffled OrderedDicts of each Digit in the given client dataset\n",
    "  # prefetch is used to fetch a set of batches (10 in this case) in order to speed up the ML processing.\n",
    "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(\n",
    "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72470bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_example_dataset = train_ds.shuffle(SHUFFLE_BUFFER, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f185d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415187b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------Rest of Centralized Model----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6304d42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /tmp/tmpusxazvrv as temporary training directory\n"
     ]
    }
   ],
   "source": [
    "model = tfdf.keras.RandomForestModel(num_trees=50, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "622406da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 19:05:17.204310: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_14' with dtype int64 and shape [699]\n",
      "\t [[{{node Placeholder/_14}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset read in 0:00:02.906207. Found 699 examples.\n",
      "Training model...\n",
      "Model trained in 0:00:00.070871\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 23-04-08 19:05:20.1411 EDT kernel.cc:1242] Loading model from path /tmp/tmpusxazvrv/model/ with prefix a4f768fc2d534889\n",
      "[INFO 23-04-08 19:05:20.1710 EDT decision_forest.cc:660] Model loaded with 50 root(s), 2702 node(s), and 14 input feature(s).\n",
      "[INFO 23-04-08 19:05:20.1711 EDT abstract_model.cc:1311] Engine \"RandomForestOptPred\" built\n",
      "[INFO 23-04-08 19:05:20.1711 EDT kernel.cc:1074] Use fast generic engine\n",
      "2023-04-08 19:05:20.189196: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype double and shape [699]\n",
      "\t [[{{node Placeholder/_6}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fb75c359d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fb75c359d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fb75c359d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model compiled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb75a59e7c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "model.fit(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c72dd253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 19:05:21.215846: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype int64 and shape [175]\n",
      "\t [[{{node Placeholder/_8}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 82ms/step\n"
     ]
    }
   ],
   "source": [
    "# Convert the testing data to a TensorFlow Dataset\n",
    "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_data)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "544f088a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
       "<div id=\"tree_plot_c8762946e8624f3fadd70134975a338c\"></div>\n",
       "<script>\n",
       "/*\n",
       " * Copyright 2021 Google LLC.\n",
       " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       " * you may not use this file except in compliance with the License.\n",
       " * You may obtain a copy of the License at\n",
       " *\n",
       " *     https://www.apache.org/licenses/LICENSE-2.0\n",
       " *\n",
       " * Unless required by applicable law or agreed to in writing, software\n",
       " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       " * See the License for the specific language governing permissions and\n",
       " * limitations under the License.\n",
       " */\n",
       "\n",
       "/**\n",
       " *  Plotting of decision trees generated by TF-DF.\n",
       " *\n",
       " *  A tree is a recursive structure of node objects.\n",
       " *  A node contains one or more of the following components:\n",
       " *\n",
       " *    - A value: Representing the output of the node. If the node is not a leaf,\n",
       " *      the value is only present for analysis i.e. it is not used for\n",
       " *      predictions.\n",
       " *\n",
       " *    - A condition : For non-leaf nodes, the condition (also known as split)\n",
       " *      defines a binary test to branch to the positive or negative child.\n",
       " *\n",
       " *    - An explanation: Generally a plot showing the relation between the label\n",
       " *      and the condition to give insights about the effect of the condition.\n",
       " *\n",
       " *    - Two children : For non-leaf nodes, the children nodes. The first\n",
       " *      children (i.e. \"node.children[0]\") is the negative children (drawn in\n",
       " *      red). The second children is the positive one (drawn in green).\n",
       " *\n",
       " */\n",
       "\n",
       "/**\n",
       " * Plots a single decision tree into a DOM element.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!tree} raw_tree Recursive tree structure.\n",
       " * @param {string} canvas_id Id of the output dom element.\n",
       " */\n",
       "function display_tree(options, raw_tree, canvas_id) {\n",
       "  console.log(options);\n",
       "\n",
       "  // Determine the node placement.\n",
       "  const tree_struct = d3.tree().nodeSize(\n",
       "      [options.node_y_offset, options.node_x_offset])(d3.hierarchy(raw_tree));\n",
       "\n",
       "  // Boundaries of the node placement.\n",
       "  let x_min = Infinity;\n",
       "  let x_max = -x_min;\n",
       "  let y_min = Infinity;\n",
       "  let y_max = -x_min;\n",
       "\n",
       "  tree_struct.each(d => {\n",
       "    if (d.x > x_max) x_max = d.x;\n",
       "    if (d.x < x_min) x_min = d.x;\n",
       "    if (d.y > y_max) y_max = d.y;\n",
       "    if (d.y < y_min) y_min = d.y;\n",
       "  });\n",
       "\n",
       "  // Size of the plot.\n",
       "  const width = y_max - y_min + options.node_x_size + options.margin * 2;\n",
       "  const height = x_max - x_min + options.node_y_size + options.margin * 2 +\n",
       "      options.node_y_offset - options.node_y_size;\n",
       "\n",
       "  const plot = d3.select(canvas_id);\n",
       "\n",
       "  // Tool tip\n",
       "  options.tooltip = plot.append('div')\n",
       "                        .attr('width', 100)\n",
       "                        .attr('height', 100)\n",
       "                        .style('padding', '4px')\n",
       "                        .style('background', '#fff')\n",
       "                        .style('box-shadow', '4px 4px 0px rgba(0,0,0,0.1)')\n",
       "                        .style('border', '1px solid black')\n",
       "                        .style('font-family', 'sans-serif')\n",
       "                        .style('font-size', options.font_size)\n",
       "                        .style('position', 'absolute')\n",
       "                        .style('z-index', '10')\n",
       "                        .attr('pointer-events', 'none')\n",
       "                        .style('display', 'none');\n",
       "\n",
       "  // Create canvas\n",
       "  const svg = plot.append('svg').attr('width', width).attr('height', height);\n",
       "  const graph =\n",
       "      svg.style('overflow', 'visible')\n",
       "          .append('g')\n",
       "          .attr('font-family', 'sans-serif')\n",
       "          .attr('font-size', options.font_size)\n",
       "          .attr(\n",
       "              'transform',\n",
       "              () => `translate(${options.margin},${\n",
       "                  - x_min + options.node_y_offset / 2 + options.margin})`);\n",
       "\n",
       "  // Plot bounding box.\n",
       "  if (options.show_plot_bounding_box) {\n",
       "    svg.append('rect')\n",
       "        .attr('width', width)\n",
       "        .attr('height', height)\n",
       "        .attr('fill', 'none')\n",
       "        .attr('stroke-width', 1.0)\n",
       "        .attr('stroke', 'black');\n",
       "  }\n",
       "\n",
       "  // Draw the edges.\n",
       "  display_edges(options, graph, tree_struct);\n",
       "\n",
       "  // Draw the nodes.\n",
       "  display_nodes(options, graph, tree_struct);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Draw the nodes of the tree.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!graph} graph D3 search handle containing the graph.\n",
       " * @param {!tree_struct} tree_struct Structure of the tree (node placement,\n",
       " *     data, etc.).\n",
       " */\n",
       "function display_nodes(options, graph, tree_struct) {\n",
       "  const nodes = graph.append('g')\n",
       "                    .selectAll('g')\n",
       "                    .data(tree_struct.descendants())\n",
       "                    .join('g')\n",
       "                    .attr('transform', d => `translate(${d.y},${d.x})`);\n",
       "\n",
       "  nodes.append('rect')\n",
       "      .attr('x', 0.5)\n",
       "      .attr('y', 0.5)\n",
       "      .attr('width', options.node_x_size)\n",
       "      .attr('height', options.node_y_size)\n",
       "      .attr('stroke', 'lightgrey')\n",
       "      .attr('stroke-width', 1)\n",
       "      .attr('fill', 'white')\n",
       "      .attr('y', -options.node_y_size / 2);\n",
       "\n",
       "  // Brackets on the right of condition nodes without children.\n",
       "  non_leaf_node_without_children =\n",
       "      nodes.filter(node => node.data.condition != null && node.children == null)\n",
       "          .append('g')\n",
       "          .attr('transform', `translate(${options.node_x_size},0)`);\n",
       "\n",
       "  non_leaf_node_without_children.append('path')\n",
       "      .attr('d', 'M0,0 C 10,0 0,10 10,10')\n",
       "      .attr('fill', 'none')\n",
       "      .attr('stroke-width', 1.0)\n",
       "      .attr('stroke', '#F00');\n",
       "\n",
       "  non_leaf_node_without_children.append('path')\n",
       "      .attr('d', 'M0,0 C 10,0 0,-10 10,-10')\n",
       "      .attr('fill', 'none')\n",
       "      .attr('stroke-width', 1.0)\n",
       "      .attr('stroke', '#0F0');\n",
       "\n",
       "  const node_content = nodes.append('g').attr(\n",
       "      'transform',\n",
       "      `translate(0,${options.node_padding - options.node_y_size / 2})`);\n",
       "\n",
       "  node_content.append(node => create_node_element(options, node));\n",
       "}\n",
       "\n",
       "/**\n",
       " * Creates the D3 content for a single node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!node} node Node to draw.\n",
       " * @return {!d3} D3 content.\n",
       " */\n",
       "function create_node_element(options, node) {\n",
       "  // Output accumulator.\n",
       "  let output = {\n",
       "    // Content to draw.\n",
       "    content: d3.create('svg:g'),\n",
       "    // Vertical offset to the next element to draw.\n",
       "    vertical_offset: 0\n",
       "  };\n",
       "\n",
       "  // Conditions.\n",
       "  if (node.data.condition != null) {\n",
       "    display_condition(options, node.data.condition, output);\n",
       "  }\n",
       "\n",
       "  // Values.\n",
       "  if (node.data.value != null) {\n",
       "    display_value(options, node.data.value, output);\n",
       "  }\n",
       "\n",
       "  // Explanations.\n",
       "  if (node.data.explanation != null) {\n",
       "    display_explanation(options, node.data.explanation, output);\n",
       "  }\n",
       "\n",
       "  return output.content.node();\n",
       "}\n",
       "\n",
       "\n",
       "/**\n",
       " * Adds a single line of text inside of a node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {string} text Text to display.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_node_text(options, text, output) {\n",
       "  output.content.append('text')\n",
       "      .attr('x', options.node_padding)\n",
       "      .attr('y', output.vertical_offset)\n",
       "      .attr('alignment-baseline', 'hanging')\n",
       "      .text(text);\n",
       "  output.vertical_offset += 10;\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds a single line of text inside of a node with a tooltip.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {string} text Text to display.\n",
       " * @param {string} tooltip Text in the Tooltip.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_node_text_with_tooltip(options, text, tooltip, output) {\n",
       "  const item = output.content.append('text')\n",
       "                   .attr('x', options.node_padding)\n",
       "                   .attr('alignment-baseline', 'hanging')\n",
       "                   .text(text);\n",
       "\n",
       "  add_tooltip(options, item, () => tooltip);\n",
       "  output.vertical_offset += 10;\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds a tooltip to a dom element.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!dom} target Dom element to equip with a tooltip.\n",
       " * @param {!func} get_content Generates the html content of the tooltip.\n",
       " */\n",
       "function add_tooltip(options, target, get_content) {\n",
       "  function show(d) {\n",
       "    options.tooltip.style('display', 'block');\n",
       "    options.tooltip.html(get_content());\n",
       "  }\n",
       "\n",
       "  function hide(d) {\n",
       "    options.tooltip.style('display', 'none');\n",
       "  }\n",
       "\n",
       "  function move(d) {\n",
       "    options.tooltip.style('display', 'block');\n",
       "    options.tooltip.style('left', (d.pageX + 5) + 'px');\n",
       "    options.tooltip.style('top', d.pageY + 'px');\n",
       "  }\n",
       "\n",
       "  target.on('mouseover', show);\n",
       "  target.on('mouseout', hide);\n",
       "  target.on('mousemove', move);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds a condition inside of a node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!condition} condition Condition to display.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_condition(options, condition, output) {\n",
       "  threshold_format = d3.format('r');\n",
       "\n",
       "  if (condition.type === 'IS_MISSING') {\n",
       "    display_node_text(options, `${condition.attribute} is missing`, output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'IS_TRUE') {\n",
       "    display_node_text(options, `${condition.attribute} is true`, output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'NUMERICAL_IS_HIGHER_THAN') {\n",
       "    format = d3.format('r');\n",
       "    display_node_text(\n",
       "        options,\n",
       "        `${condition.attribute} >= ${threshold_format(condition.threshold)}`,\n",
       "        output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'CATEGORICAL_IS_IN') {\n",
       "    display_node_text_with_tooltip(\n",
       "        options, `${condition.attribute} in [...]`,\n",
       "        `${condition.attribute} in [${condition.mask}]`, output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'CATEGORICAL_SET_CONTAINS') {\n",
       "    display_node_text_with_tooltip(\n",
       "        options, `${condition.attribute} intersect [...]`,\n",
       "        `${condition.attribute} intersect [${condition.mask}]`, output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'NUMERICAL_SPARSE_OBLIQUE') {\n",
       "    display_node_text_with_tooltip(\n",
       "        options, `Sparse oblique split...`,\n",
       "        `[${condition.attributes}]*[${condition.weights}]>=${\n",
       "            threshold_format(condition.threshold)}`,\n",
       "        output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  display_node_text(\n",
       "      options, `Non supported condition ${condition.type}`, output);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds a value inside of a node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!value} value Value to display.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_value(options, value, output) {\n",
       "  if (value.type === 'PROBABILITY') {\n",
       "    const left_margin = 0;\n",
       "    const right_margin = 50;\n",
       "    const plot_width = options.node_x_size - options.node_padding * 2 -\n",
       "        left_margin - right_margin;\n",
       "\n",
       "    let cusum = Array.from(d3.cumsum(value.distribution));\n",
       "    cusum.unshift(0);\n",
       "    const distribution_plot = output.content.append('g').attr(\n",
       "        'transform', `translate(0,${output.vertical_offset + 0.5})`);\n",
       "\n",
       "    distribution_plot.selectAll('rect')\n",
       "        .data(value.distribution)\n",
       "        .join('rect')\n",
       "        .attr('height', 10)\n",
       "        .attr(\n",
       "            'x',\n",
       "            (d, i) =>\n",
       "                (cusum[i] * plot_width + left_margin + options.node_padding))\n",
       "        .attr('width', (d, i) => d * plot_width)\n",
       "        .style('fill', (d, i) => d3.schemeSet1[i]);\n",
       "\n",
       "    const num_examples =\n",
       "        output.content.append('g')\n",
       "            .attr('transform', `translate(0,${output.vertical_offset})`)\n",
       "            .append('text')\n",
       "            .attr('x', options.node_x_size - options.node_padding)\n",
       "            .attr('alignment-baseline', 'hanging')\n",
       "            .attr('text-anchor', 'end')\n",
       "            .text(`(${value.num_examples})`);\n",
       "\n",
       "    const distribution_details = d3.create('ul');\n",
       "    distribution_details.selectAll('li')\n",
       "        .data(value.distribution)\n",
       "        .join('li')\n",
       "        .append('span')\n",
       "        .text(\n",
       "            (d, i) =>\n",
       "                'class ' + i + ': ' + d3.format('.3%')(value.distribution[i]));\n",
       "\n",
       "    add_tooltip(options, distribution_plot, () => distribution_details.html());\n",
       "    add_tooltip(options, num_examples, () => 'Number of examples');\n",
       "\n",
       "    output.vertical_offset += 10;\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (value.type === 'REGRESSION') {\n",
       "    display_node_text(\n",
       "        options,\n",
       "        'value: ' + d3.format('r')(value.value) + ` (` +\n",
       "            d3.format('.6')(value.num_examples) + `)`,\n",
       "        output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  display_node_text(options, `Non supported value ${value.type}`, output);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds an explanation inside of a node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!explanation} explanation Explanation to display.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_explanation(options, explanation, output) {\n",
       "  // Margin before the explanation.\n",
       "  output.vertical_offset += 10;\n",
       "\n",
       "  display_node_text(\n",
       "      options, `Non supported explanation ${explanation.type}`, output);\n",
       "}\n",
       "\n",
       "\n",
       "/**\n",
       " * Draw the edges of the tree.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!graph} graph D3 search handle containing the graph.\n",
       " * @param {!tree_struct} tree_struct Structure of the tree (node placement,\n",
       " *     data, etc.).\n",
       " */\n",
       "function display_edges(options, graph, tree_struct) {\n",
       "  // Draw an edge between a parent and a child node with a bezier.\n",
       "  function draw_single_edge(d) {\n",
       "    return 'M' + (d.source.y + options.node_x_size) + ',' + d.source.x + ' C' +\n",
       "        (d.source.y + options.node_x_size + options.edge_rounding) + ',' +\n",
       "        d.source.x + ' ' + (d.target.y - options.edge_rounding) + ',' +\n",
       "        d.target.x + ' ' + d.target.y + ',' + d.target.x;\n",
       "  }\n",
       "\n",
       "  graph.append('g')\n",
       "      .attr('fill', 'none')\n",
       "      .attr('stroke-width', 1.2)\n",
       "      .selectAll('path')\n",
       "      .data(tree_struct.links())\n",
       "      .join('path')\n",
       "      .attr('d', draw_single_edge)\n",
       "      .attr(\n",
       "          'stroke', d => (d.target === d.source.children[0]) ? '#0F0' : '#F00');\n",
       "}\n",
       "\n",
       "display_tree({\"margin\": 10, \"node_x_size\": 160, \"node_y_size\": 28, \"node_x_offset\": 180, \"node_y_offset\": 33, \"font_size\": 10, \"edge_rounding\": 20, \"node_padding\": 2, \"show_plot_bounding_box\": false}, {\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.9384835479256081, 0.06151645207439199], \"num_examples\": 699.0}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"Level.ethnic\", \"threshold\": 4.5}, \"children\": [{\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [1.0, 0.0], \"num_examples\": 69.0}}, {\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.9317460317460318, 0.06825396825396825], \"num_examples\": 630.0}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"CombWords\", \"threshold\": 0.9769648313522339}, \"children\": [{\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.9448160535117057, 0.05518394648829431], \"num_examples\": 598.0}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"HC\", \"threshold\": 0.13080863654613495}}, {\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.6875, 0.3125], \"num_examples\": 32.0}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"Level.gender\", \"threshold\": 0.5}}]}]}, \"#tree_plot_c8762946e8624f3fadd70134975a338c\")\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf.model_plotter.plot_model_in_colab(model, tree_idx=0, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "506098d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"random_forest_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      "=================================================================\n",
      "Total params: 1\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n",
      "Type: \"RANDOM_FOREST\"\n",
      "Task: CLASSIFICATION\n",
      "Label: \"__LABEL\"\n",
      "\n",
      "Input Features (14):\n",
      "\tBetween\n",
      "\tCombWords\n",
      "\tDegree\n",
      "\tFamHist\n",
      "\tGCC\n",
      "\tGramComplex\n",
      "\tHC\n",
      "\tLevel.ethnic\n",
      "\tLevel.fed\n",
      "\tLevel.gender\n",
      "\tLevel.income\n",
      "\tLevel.med\n",
      "\tPathLength\n",
      "\tPercentile\n",
      "\n",
      "No weights\n",
      "\n",
      "Variable Importance: INV_MEAN_MIN_DEPTH:\n",
      "    1.   \"Percentile\"  0.280091 ################\n",
      "    2.    \"CombWords\"  0.236790 ##########\n",
      "    3.  \"GramComplex\"  0.218871 ########\n",
      "    4.          \"GCC\"  0.192062 #####\n",
      "    5.           \"HC\"  0.189847 #####\n",
      "    6.       \"Degree\"  0.188856 #####\n",
      "    7.      \"Between\"  0.185558 ####\n",
      "    8.   \"PathLength\"  0.183286 ####\n",
      "    9.      \"FamHist\"  0.157640 #\n",
      "   10.    \"Level.fed\"  0.155451 #\n",
      "   11. \"Level.income\"  0.150453 \n",
      "   12.    \"Level.med\"  0.149143 \n",
      "   13. \"Level.ethnic\"  0.147937 \n",
      "   14. \"Level.gender\"  0.146763 \n",
      "\n",
      "Variable Importance: NUM_AS_ROOT:\n",
      "    1.   \"Percentile\" 14.000000 ################\n",
      "    2.  \"GramComplex\" 11.000000 ############\n",
      "    3.    \"CombWords\"  8.000000 ########\n",
      "    4.      \"Between\"  4.000000 ###\n",
      "    5.       \"Degree\"  4.000000 ###\n",
      "    6.          \"GCC\"  3.000000 ##\n",
      "    7.           \"HC\"  2.000000 #\n",
      "    8.   \"PathLength\"  2.000000 #\n",
      "    9. \"Level.ethnic\"  1.000000 \n",
      "   10.    \"Level.fed\"  1.000000 \n",
      "\n",
      "Variable Importance: NUM_NODES:\n",
      "    1.   \"Percentile\" 165.000000 ################\n",
      "    2.          \"GCC\" 162.000000 ###############\n",
      "    3.           \"HC\" 161.000000 ###############\n",
      "    4.   \"PathLength\" 149.000000 ##############\n",
      "    5.      \"Between\" 145.000000 #############\n",
      "    6.       \"Degree\" 128.000000 ############\n",
      "    7.  \"GramComplex\" 115.000000 ##########\n",
      "    8.    \"CombWords\" 79.000000 #######\n",
      "    9.    \"Level.fed\" 64.000000 #####\n",
      "   10.    \"Level.med\" 46.000000 ###\n",
      "   11. \"Level.income\" 42.000000 ###\n",
      "   12.      \"FamHist\" 32.000000 ##\n",
      "   13. \"Level.gender\" 26.000000 #\n",
      "   14. \"Level.ethnic\" 12.000000 \n",
      "\n",
      "Variable Importance: SUM_SCORE:\n",
      "    1.   \"Percentile\" 833.084945 ################\n",
      "    2.          \"GCC\" 659.037662 ############\n",
      "    3.           \"HC\" 546.396291 ##########\n",
      "    4.      \"Between\" 536.133434 ##########\n",
      "    5.   \"PathLength\" 529.098066 #########\n",
      "    6.    \"CombWords\" 518.276923 #########\n",
      "    7.  \"GramComplex\" 507.698141 #########\n",
      "    8.       \"Degree\" 475.587350 ########\n",
      "    9.    \"Level.fed\" 208.439711 ###\n",
      "   10.    \"Level.med\" 176.902330 ##\n",
      "   11. \"Level.income\" 163.437563 ##\n",
      "   12.      \"FamHist\" 119.529781 #\n",
      "   13. \"Level.gender\" 77.396663 \n",
      "   14. \"Level.ethnic\" 38.540303 \n",
      "\n",
      "\n",
      "\n",
      "Winner takes all: true\n",
      "Out-of-bag evaluation: accuracy:0.932761 logloss:1.3402\n",
      "Number of trees: 50\n",
      "Total number of nodes: 2702\n",
      "\n",
      "Number of nodes by tree:\n",
      "Count: 50 Average: 54.04 StdDev: 7.19155\n",
      "Min: 27 Max: 65 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 27, 28) 1   2.00%   2.00% #\n",
      "[ 28, 30) 0   0.00%   2.00%\n",
      "[ 30, 32) 0   0.00%   2.00%\n",
      "[ 32, 34) 0   0.00%   2.00%\n",
      "[ 34, 36) 1   2.00%   4.00% #\n",
      "[ 36, 38) 0   0.00%   4.00%\n",
      "[ 38, 40) 0   0.00%   4.00%\n",
      "[ 40, 42) 0   0.00%   4.00%\n",
      "[ 42, 44) 2   4.00%   8.00% ##\n",
      "[ 44, 46) 1   2.00%  10.00% #\n",
      "[ 46, 48) 2   4.00%  14.00% ##\n",
      "[ 48, 50) 5  10.00%  24.00% ######\n",
      "[ 50, 52) 2   4.00%  28.00% ##\n",
      "[ 52, 54) 9  18.00%  46.00% ##########\n",
      "[ 54, 56) 5  10.00%  56.00% ######\n",
      "[ 56, 58) 8  16.00%  72.00% #########\n",
      "[ 58, 60) 5  10.00%  82.00% ######\n",
      "[ 60, 62) 4   8.00%  90.00% ####\n",
      "[ 62, 64) 2   4.00%  94.00% ##\n",
      "[ 64, 65] 3   6.00% 100.00% ###\n",
      "\n",
      "Depth by leafs:\n",
      "Count: 1376 Average: 6.05451 StdDev: 1.98265\n",
      "Min: 1 Max: 9 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 1, 2)   7   0.51%   0.51%\n",
      "[ 2, 3)  30   2.18%   2.69% #\n",
      "[ 3, 4) 111   8.07%  10.76% #####\n",
      "[ 4, 5) 182  13.23%  23.98% #######\n",
      "[ 5, 6) 246  17.88%  41.86% ##########\n",
      "[ 6, 7) 217  15.77%  57.63% #########\n",
      "[ 7, 8) 209  15.19%  72.82% ########\n",
      "[ 8, 9) 158  11.48%  84.30% ######\n",
      "[ 9, 9] 216  15.70% 100.00% #########\n",
      "\n",
      "Number of training obs by leaf:\n",
      "Count: 1376 Average: 25.3997 StdDev: 39.1808\n",
      "Min: 5 Max: 309 Ignored: 0\n",
      "----------------------------------------------\n",
      "[   5,  20) 999  72.60%  72.60% ##########\n",
      "[  20,  35) 101   7.34%  79.94% #\n",
      "[  35,  50)  67   4.87%  84.81% #\n",
      "[  50,  66)  47   3.42%  88.23%\n",
      "[  66,  81)  26   1.89%  90.12%\n",
      "[  81,  96)  30   2.18%  92.30%\n",
      "[  96, 111)  31   2.25%  94.55%\n",
      "[ 111, 127)  28   2.03%  96.58%\n",
      "[ 127, 142)  10   0.73%  97.31%\n",
      "[ 142, 157)   6   0.44%  97.75%\n",
      "[ 157, 172)  11   0.80%  98.55%\n",
      "[ 172, 188)   8   0.58%  99.13%\n",
      "[ 188, 203)   6   0.44%  99.56%\n",
      "[ 203, 218)   2   0.15%  99.71%\n",
      "[ 218, 233)   0   0.00%  99.71%\n",
      "[ 233, 249)   0   0.00%  99.71%\n",
      "[ 249, 264)   1   0.07%  99.78%\n",
      "[ 264, 279)   1   0.07%  99.85%\n",
      "[ 279, 294)   0   0.00%  99.85%\n",
      "[ 294, 309]   2   0.15% 100.00%\n",
      "\n",
      "Attribute in nodes:\n",
      "\t165 : Percentile [NUMERICAL]\n",
      "\t162 : GCC [NUMERICAL]\n",
      "\t161 : HC [NUMERICAL]\n",
      "\t149 : PathLength [NUMERICAL]\n",
      "\t145 : Between [NUMERICAL]\n",
      "\t128 : Degree [NUMERICAL]\n",
      "\t115 : GramComplex [NUMERICAL]\n",
      "\t79 : CombWords [NUMERICAL]\n",
      "\t64 : Level.fed [NUMERICAL]\n",
      "\t46 : Level.med [NUMERICAL]\n",
      "\t42 : Level.income [NUMERICAL]\n",
      "\t32 : FamHist [NUMERICAL]\n",
      "\t26 : Level.gender [NUMERICAL]\n",
      "\t12 : Level.ethnic [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 0:\n",
      "\t14 : Percentile [NUMERICAL]\n",
      "\t11 : GramComplex [NUMERICAL]\n",
      "\t8 : CombWords [NUMERICAL]\n",
      "\t4 : Degree [NUMERICAL]\n",
      "\t4 : Between [NUMERICAL]\n",
      "\t3 : GCC [NUMERICAL]\n",
      "\t2 : PathLength [NUMERICAL]\n",
      "\t2 : HC [NUMERICAL]\n",
      "\t1 : Level.fed [NUMERICAL]\n",
      "\t1 : Level.ethnic [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 1:\n",
      "\t31 : Percentile [NUMERICAL]\n",
      "\t20 : CombWords [NUMERICAL]\n",
      "\t19 : GramComplex [NUMERICAL]\n",
      "\t14 : Between [NUMERICAL]\n",
      "\t13 : GCC [NUMERICAL]\n",
      "\t13 : Degree [NUMERICAL]\n",
      "\t10 : PathLength [NUMERICAL]\n",
      "\t10 : HC [NUMERICAL]\n",
      "\t6 : FamHist [NUMERICAL]\n",
      "\t3 : Level.income [NUMERICAL]\n",
      "\t2 : Level.fed [NUMERICAL]\n",
      "\t1 : Level.gender [NUMERICAL]\n",
      "\t1 : Level.ethnic [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 2:\n",
      "\t50 : Percentile [NUMERICAL]\n",
      "\t36 : HC [NUMERICAL]\n",
      "\t34 : Between [NUMERICAL]\n",
      "\t33 : CombWords [NUMERICAL]\n",
      "\t30 : GramComplex [NUMERICAL]\n",
      "\t30 : GCC [NUMERICAL]\n",
      "\t26 : PathLength [NUMERICAL]\n",
      "\t25 : Degree [NUMERICAL]\n",
      "\t11 : FamHist [NUMERICAL]\n",
      "\t7 : Level.fed [NUMERICAL]\n",
      "\t6 : Level.income [NUMERICAL]\n",
      "\t4 : Level.med [NUMERICAL]\n",
      "\t4 : Level.ethnic [NUMERICAL]\n",
      "\t3 : Level.gender [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 3:\n",
      "\t69 : Percentile [NUMERICAL]\n",
      "\t58 : HC [NUMERICAL]\n",
      "\t54 : Between [NUMERICAL]\n",
      "\t49 : GCC [NUMERICAL]\n",
      "\t47 : CombWords [NUMERICAL]\n",
      "\t46 : Degree [NUMERICAL]\n",
      "\t45 : PathLength [NUMERICAL]\n",
      "\t44 : GramComplex [NUMERICAL]\n",
      "\t21 : Level.fed [NUMERICAL]\n",
      "\t19 : Level.income [NUMERICAL]\n",
      "\t18 : Level.med [NUMERICAL]\n",
      "\t18 : FamHist [NUMERICAL]\n",
      "\t7 : Level.ethnic [NUMERICAL]\n",
      "\t5 : Level.gender [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 5:\n",
      "\t119 : Percentile [NUMERICAL]\n",
      "\t107 : HC [NUMERICAL]\n",
      "\t102 : GCC [NUMERICAL]\n",
      "\t101 : PathLength [NUMERICAL]\n",
      "\t96 : Between [NUMERICAL]\n",
      "\t85 : GramComplex [NUMERICAL]\n",
      "\t83 : Degree [NUMERICAL]\n",
      "\t63 : CombWords [NUMERICAL]\n",
      "\t41 : Level.fed [NUMERICAL]\n",
      "\t33 : Level.med [NUMERICAL]\n",
      "\t31 : Level.income [NUMERICAL]\n",
      "\t28 : FamHist [NUMERICAL]\n",
      "\t16 : Level.gender [NUMERICAL]\n",
      "\t9 : Level.ethnic [NUMERICAL]\n",
      "\n",
      "Condition type in nodes:\n",
      "\t1326 : HigherCondition\n",
      "Condition type in nodes with depth <= 0:\n",
      "\t50 : HigherCondition\n",
      "Condition type in nodes with depth <= 1:\n",
      "\t143 : HigherCondition\n",
      "Condition type in nodes with depth <= 2:\n",
      "\t299 : HigherCondition\n",
      "Condition type in nodes with depth <= 3:\n",
      "\t500 : HigherCondition\n",
      "Condition type in nodes with depth <= 5:\n",
      "\t914 : HigherCondition\n",
      "Node format: NOT_SET\n",
      "\n",
      "Training OOB:\n",
      "\ttrees: 1, Out-of-bag evaluation: accuracy:0.888889 logloss:4.00485\n",
      "\ttrees: 11, Out-of-bag evaluation: accuracy:0.932277 logloss:1.97407\n",
      "\ttrees: 24, Out-of-bag evaluation: accuracy:0.934192 logloss:1.62166\n",
      "\ttrees: 34, Out-of-bag evaluation: accuracy:0.935622 logloss:1.38467\n",
      "\ttrees: 45, Out-of-bag evaluation: accuracy:0.935622 logloss:1.38765\n",
      "\ttrees: 50, Out-of-bag evaluation: accuracy:0.932761 logloss:1.3402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adf680ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/draxtik20/Federated_Learning_CDI/venv/lib/python3.9/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 518, in test_function  *\n        return step_function_trained(self, iterator)\n    File \"/home/draxtik20/Federated_Learning_CDI/venv/lib/python3.9/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 497, in run_step  *\n        outputs = model.test_step(data)\n    File \"/home/draxtik20/Federated_Learning_CDI/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1788, in test_step  **\n        y_pred = self(x, training=False)\n    File \"/home/draxtik20/Federated_Learning_CDI/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filejdqiy1vn.py\", line 104, in tf__call\n        ag__.if_stmt(ag__.ld(self)._semantics is None, if_body_2, else_body_2, get_state_3, set_state_3, ('do_return', 'retval_'), 2)\n    File \"/tmp/__autograph_generated_filejdqiy1vn.py\", line 43, in else_body_2\n        normalized_inputs = ag__.converted_call(ag__.ld(self)._build_normalized_inputs, (ag__.ld(inputs),), None, fscope)\n    File \"/tmp/__autograph_generated_fileu3gnez9q.py\", line 97, in tf___build_normalized_inputs\n        ag__.if_stmt(ag__.converted_call(ag__.ld(isinstance), (ag__.ld(inputs), ag__.ld(dict)), None, fscope), if_body_4, else_body_4, get_state_4, set_state_4, ('inputs',), 1)\n    File \"/tmp/__autograph_generated_fileu3gnez9q.py\", line 96, in else_body_4\n        ag__.if_stmt(ag__.converted_call(ag__.ld(isinstance), (ag__.ld(inputs), ag__.ld(tf).Tensor), None, fscope), if_body_3, else_body_3, get_state_3, set_state_3, ('inputs',), 1)\n    File \"/tmp/__autograph_generated_fileu3gnez9q.py\", line 75, in if_body_3\n        ag__.if_stmt(ag__.converted_call(ag__.ld(len), (ag__.ld(self)._semantics,), None, fscope) != 1, if_body_1, else_body_1, get_state_1, set_state_1, (), 0)\n    File \"/tmp/__autograph_generated_fileu3gnez9q.py\", line 71, in if_body_1\n        raise ag__.converted_call(ag__.ld(ValueError), (f'Calling model with input shape different from the input shape provided during training: Feeding a single array {ag__.ld(inputs)} while the model was trained on {ag__.ld(self)._semantics}.',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'random_forest_model' (type RandomForestModel).\n    \n    in user code:\n    \n        File \"/home/draxtik20/Federated_Learning_CDI/venv/lib/python3.9/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 644, in call  *\n            normalized_inputs = self._build_normalized_inputs(inputs)\n        File \"/home/draxtik20/Federated_Learning_CDI/venv/lib/python3.9/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 586, in _build_normalized_inputs  *\n            raise ValueError(\n    \n        ValueError: Calling model with input shape different from the input shape provided during training: Feeding a single array Tensor(\"inputs:0\", shape=(None, 15), dtype=float32) while the model was trained on {'Level.gender': <Semantic.NUMERICAL: 1>, 'Level.ethnic': <Semantic.NUMERICAL: 1>, 'Level.med': <Semantic.NUMERICAL: 1>, 'Level.fed': <Semantic.NUMERICAL: 1>, 'Level.income': <Semantic.NUMERICAL: 1>, 'FamHist': <Semantic.NUMERICAL: 1>, 'Percentile': <Semantic.NUMERICAL: 1>, 'GCC': <Semantic.NUMERICAL: 1>, 'Degree': <Semantic.NUMERICAL: 1>, 'HC': <Semantic.NUMERICAL: 1>, 'Between': <Semantic.NUMERICAL: 1>, 'PathLength': <Semantic.NUMERICAL: 1>, 'CombWords': <Semantic.NUMERICAL: 1>, 'GramComplex': <Semantic.NUMERICAL: 1>}.\n    \n    \n    Call arguments received by layer 'random_forest_model' (type RandomForestModel):\n      • inputs=tf.Tensor(shape=(None, 15), dtype=float32)\n      • training=False\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m evaluation\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Federated_Learning_CDI/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file4hawtnmd.py:16\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function_trained), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filelep3903z.py:41\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__step_function_trained\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fscope_1\u001b[38;5;241m.\u001b[39mret(retval__1, do_return_1)\n\u001b[1;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mnext\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(iterator),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 41\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun, (ag__\u001b[38;5;241m.\u001b[39mld(run_step),), \u001b[38;5;28mdict\u001b[39m(args\u001b[38;5;241m=\u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(data),)), fscope)\n\u001b[1;32m     42\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(_reduce_per_replica), (ag__\u001b[38;5;241m.\u001b[39mld(outputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdistribute_strategy), \u001b[38;5;28mdict\u001b[39m(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m), fscope)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filelep3903z.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__step_function_trained.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     28\u001b[0m do_return_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     29\u001b[0m retval__1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mtest_step, (ag__\u001b[38;5;241m.\u001b[39mld(data),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcontrol_dependencies(ag__\u001b[38;5;241m.\u001b[39mld(_minimum_control_deps)(ag__\u001b[38;5;241m.\u001b[39mld(outputs))):\n\u001b[1;32m     32\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39m_test_counter\u001b[38;5;241m.\u001b[39massign_add, (\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filejdqiy1vn.py:104\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    102\u001b[0m multitask_item \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultitask_item\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    103\u001b[0m sub_pred \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub_pred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_semantics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, if_body_2, else_body_2, get_state_3, set_state_3, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_return\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretval_\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fscope\u001b[38;5;241m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filejdqiy1vn.py:43\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body_2\u001b[39m():\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m retval_, do_return\n\u001b[0;32m---> 43\u001b[0m     normalized_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_normalized_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileu3gnez9q.py:97\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___build_normalized_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mor_(\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28misinstance\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlist\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), \u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28misinstance\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mtuple\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     96\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28misinstance\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mTensor), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28misinstance\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mdict\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), if_body_4, else_body_4, get_state_4, set_state_4, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     98\u001b[0m semantic_inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf_core)\u001b[38;5;241m.\u001b[39mcombine_tensors_and_semantics, (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_semantics), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     99\u001b[0m normalized_semantic_inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf_core)\u001b[38;5;241m.\u001b[39mnormalize_inputs, (ag__\u001b[38;5;241m.\u001b[39mld(semantic_inputs),), \u001b[38;5;28mdict\u001b[39m(categorical_integer_offset_correction\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mnot_(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_advanced_arguments\u001b[38;5;241m.\u001b[39mdisable_categorical_integer_offset_correction)), fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileu3gnez9q.py:96\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___build_normalized_inputs.<locals>.else_body_4\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;167;01mValueError\u001b[39;00m), (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe inference input tensor is expected to be a tensor, list of tensors or a dictionary of tensors. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mld(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     95\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mor_(\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28misinstance\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlist\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), \u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28misinstance\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mtuple\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileu3gnez9q.py:75\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___build_normalized_inputs.<locals>.else_body_4.<locals>.if_body_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body_1\u001b[39m():\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_semantics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mnext\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28miter\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_semantics\u001b[38;5;241m.\u001b[39mkeys, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope): ag__\u001b[38;5;241m.\u001b[39mld(inputs)}\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileu3gnez9q.py:71\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___build_normalized_inputs.<locals>.else_body_4.<locals>.if_body_3.<locals>.if_body_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mif_body_1\u001b[39m():\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;167;01mValueError\u001b[39;00m), (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalling model with input shape different from the input shape provided during training: Feeding a single array \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mld(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m while the model was trained on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_semantics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/draxtik20/Federated_Learning_CDI/venv/lib/python3.9/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 518, in test_function  *\n        return step_function_trained(self, iterator)\n    File \"/home/draxtik20/Federated_Learning_CDI/venv/lib/python3.9/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 497, in run_step  *\n        outputs = model.test_step(data)\n    File \"/home/draxtik20/Federated_Learning_CDI/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1788, in test_step  **\n        y_pred = self(x, training=False)\n    File \"/home/draxtik20/Federated_Learning_CDI/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filejdqiy1vn.py\", line 104, in tf__call\n        ag__.if_stmt(ag__.ld(self)._semantics is None, if_body_2, else_body_2, get_state_3, set_state_3, ('do_return', 'retval_'), 2)\n    File \"/tmp/__autograph_generated_filejdqiy1vn.py\", line 43, in else_body_2\n        normalized_inputs = ag__.converted_call(ag__.ld(self)._build_normalized_inputs, (ag__.ld(inputs),), None, fscope)\n    File \"/tmp/__autograph_generated_fileu3gnez9q.py\", line 97, in tf___build_normalized_inputs\n        ag__.if_stmt(ag__.converted_call(ag__.ld(isinstance), (ag__.ld(inputs), ag__.ld(dict)), None, fscope), if_body_4, else_body_4, get_state_4, set_state_4, ('inputs',), 1)\n    File \"/tmp/__autograph_generated_fileu3gnez9q.py\", line 96, in else_body_4\n        ag__.if_stmt(ag__.converted_call(ag__.ld(isinstance), (ag__.ld(inputs), ag__.ld(tf).Tensor), None, fscope), if_body_3, else_body_3, get_state_3, set_state_3, ('inputs',), 1)\n    File \"/tmp/__autograph_generated_fileu3gnez9q.py\", line 75, in if_body_3\n        ag__.if_stmt(ag__.converted_call(ag__.ld(len), (ag__.ld(self)._semantics,), None, fscope) != 1, if_body_1, else_body_1, get_state_1, set_state_1, (), 0)\n    File \"/tmp/__autograph_generated_fileu3gnez9q.py\", line 71, in if_body_1\n        raise ag__.converted_call(ag__.ld(ValueError), (f'Calling model with input shape different from the input shape provided during training: Feeding a single array {ag__.ld(inputs)} while the model was trained on {ag__.ld(self)._semantics}.',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'random_forest_model' (type RandomForestModel).\n    \n    in user code:\n    \n        File \"/home/draxtik20/Federated_Learning_CDI/venv/lib/python3.9/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 644, in call  *\n            normalized_inputs = self._build_normalized_inputs(inputs)\n        File \"/home/draxtik20/Federated_Learning_CDI/venv/lib/python3.9/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 586, in _build_normalized_inputs  *\n            raise ValueError(\n    \n        ValueError: Calling model with input shape different from the input shape provided during training: Feeding a single array Tensor(\"inputs:0\", shape=(None, 15), dtype=float32) while the model was trained on {'Level.gender': <Semantic.NUMERICAL: 1>, 'Level.ethnic': <Semantic.NUMERICAL: 1>, 'Level.med': <Semantic.NUMERICAL: 1>, 'Level.fed': <Semantic.NUMERICAL: 1>, 'Level.income': <Semantic.NUMERICAL: 1>, 'FamHist': <Semantic.NUMERICAL: 1>, 'Percentile': <Semantic.NUMERICAL: 1>, 'GCC': <Semantic.NUMERICAL: 1>, 'Degree': <Semantic.NUMERICAL: 1>, 'HC': <Semantic.NUMERICAL: 1>, 'Between': <Semantic.NUMERICAL: 1>, 'PathLength': <Semantic.NUMERICAL: 1>, 'CombWords': <Semantic.NUMERICAL: 1>, 'GramComplex': <Semantic.NUMERICAL: 1>}.\n    \n    \n    Call arguments received by layer 'random_forest_model' (type RandomForestModel):\n      • inputs=tf.Tensor(shape=(None, 15), dtype=float32)\n      • training=False\n"
     ]
    }
   ],
   "source": [
    "model.compile(metrics=[\"accuracy\"])\n",
    "evaluation = model.evaluate(test_data, return_dict=True)\n",
    "print()\n",
    "\n",
    "for name, value in evaluation.items():\n",
    "  print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42831e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
